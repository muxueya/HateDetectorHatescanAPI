{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYJ/t7kTIpvU1zuOkXO1c2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muxueya/HateDetectorHatescanAPI/blob/main/HateDetectorHatescanAPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY8LqFo3jg_c",
        "outputId": "14e14a89-b86d-41ab-874c-fda7d5719509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: krippendorff in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from krippendorff) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install Required Libraries\n",
        "!pip install krippendorff scikit-learn pandas openpyxl\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import Libraries\n",
        "import requests\n",
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "import re\n",
        "import krippendorff\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n"
      ],
      "metadata": {
        "id": "z9U-oO9VkG9Q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define Function to Get Toxic Probability from Hatescan API\n",
        "def get_toxic_probability(text, language='en', flag_detect_lang='False'):\n",
        "    # Prepare the payload\n",
        "    text = text.replace('\"', '\\\\\"')\n",
        "    payload = '{\"text\": \"' + text + '\", \"language\": \"' + language + '\" , \"flag_detect_lang\": \"' + flag_detect_lang + '\"}'\n",
        "    json_payload = json.loads(payload, strict=False)\n",
        "\n",
        "    # Headers for the API request\n",
        "    headers = {\"Content-Type\": \"application/json; charset=utf-8\"}\n",
        "\n",
        "    # Hatescan API URL (updated URL)\n",
        "    api_hatescan_url = 'https://api.hatescan.com/predict/toxic'\n",
        "\n",
        "    # Send a POST request to the Hatescan API\n",
        "    hatescan_response = requests.post(api_hatescan_url, headers=headers, json=json_payload)\n",
        "\n",
        "    # Return the API response as JSON\n",
        "    return hatescan_response.json()\n",
        "\n",
        "# Step 4: Define Function to Read Comments from Excel File\n",
        "def read_comments_from_excel(filename):\n",
        "    # Load the comments from an Excel file (assuming the column name is 'comment')\n",
        "    df = pd.read_excel(filename)\n",
        "    comments = df['comment'].astype(str).tolist()  # Convert comments to a list of strings\n",
        "    return comments\n",
        "\n"
      ],
      "metadata": {
        "id": "BTj1UsGakNGn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Step 5: Process Comments for Toxicity Detection\n",
        "# def process_comments(input_file, output_file, language='en', flag_detect_lang='False'):\n",
        "#     # Read comments from the Excel file\n",
        "#     comments = read_comments_from_excel(input_file)\n",
        "#     num_comments = len(comments)\n",
        "\n",
        "#     # Prepare the result list\n",
        "#     results = []\n",
        "\n",
        "#     # Loop through each comment and get toxicity scores\n",
        "#     for idx, comment in enumerate(comments, 1):\n",
        "#         print(f\"Processing comment {idx}/{num_comments}: {comment[:100]}\")\n",
        "#         try:\n",
        "#             toxic_score = get_toxic_probability(comment, language, flag_detect_lang)\n",
        "\n",
        "#             # Correct key to access predictions\n",
        "#             toxic_predictions = toxic_score.get('predictions', 'N/A')\n",
        "\n",
        "#             # Output the results to the terminal\n",
        "#             print(f\"Toxic probability score: {toxic_predictions}%\\n\")\n",
        "\n",
        "#             # Append the comment and toxic probabilities to the result list\n",
        "#             results.append({\n",
        "#                 'comment': comment,\n",
        "#                 'toxic_predictions': toxic_predictions\n",
        "#             })\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error processing comment: {comment}, Error: {e}\")\n",
        "\n",
        "#     # Save the results to a CSV file\n",
        "#     with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "#         fieldnames = ['comment', 'toxic_predictions']\n",
        "#         writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "#         writer.writeheader()\n",
        "#         writer.writerows(results)\n",
        "\n",
        "#     print(f\"Results saved to {output_file}\")\n",
        "\n",
        "# # Usage Example\n",
        "# input_file = 'incels-5000.xlsx'  # Replace with your actual file path\n",
        "# output_file = 'hatescan_results.csv'\n",
        "# process_comments(input_file, output_file)\n",
        "\n"
      ],
      "metadata": {
        "id": "K82AlZSykWHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Step 6: Sample Comments with High and Low Toxicity\n",
        "# def select_comments(input_file, high_output_file, low_output_file, X, Y):\n",
        "#     # Load the toxicity results\n",
        "#     df = pd.read_csv(input_file)\n",
        "\n",
        "#     # Convert 'toxic_predictions' column to numeric\n",
        "#     df['toxic_predictions'] = pd.to_numeric(df['toxic_predictions'], errors='coerce')\n",
        "\n",
        "#     # Filter for high and low toxicity comments\n",
        "#     high_toxic = df[df['toxic_predictions'] >= 50]\n",
        "#     low_toxic = df[df['toxic_predictions'] < 50]\n",
        "\n",
        "#     # Randomly select X and Y comments\n",
        "#     selected_high_toxic = high_toxic.sample(n=X, random_state=42)\n",
        "#     selected_low_toxic = low_toxic.sample(n=Y, random_state=42)\n",
        "\n",
        "#     # Save the high-toxicity comments to a CSV file\n",
        "#     selected_high_toxic.to_csv(high_output_file, index=False, encoding='utf-8')\n",
        "\n",
        "#     # Save the low-toxicity comments to a separate CSV file\n",
        "#     selected_low_toxic.to_csv(low_output_file, index=False, encoding='utf-8')\n",
        "\n",
        "#     print(f\"Randomly selected {X} high-toxicity comments saved to {high_output_file}\")\n",
        "#     print(f\"Randomly selected {Y} low-toxicity comments saved to {low_output_file}\")\n",
        "\n",
        "# # Usage Example\n",
        "# select_comments('hatescan_results.csv', 'high_toxic_comments.csv', 'low_toxic_comments.csv', X=10, Y=10)\n",
        "\n"
      ],
      "metadata": {
        "id": "RDYCl21wkcWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Calculate Krippendorff's Alpha for Annotators\n",
        "def calculate_krippendorff_alpha(file):\n",
        "    df = pd.read_excel(file)\n",
        "    annotations = df[['a', 'b', 'c', 'd', 'e']].values.T  # Transpose the data for Krippendorff's alpha\n",
        "    alpha = krippendorff.alpha(reliability_data=annotations, level_of_measurement='nominal')\n",
        "    print(f\" {file} Krippendorff's Alpha: {alpha:.3f}\")\n",
        "\n",
        "# Example Usage\n",
        "calculate_krippendorff_alpha('Sample_high_toxic_comments.xlsx')\n",
        "calculate_krippendorff_alpha('Sample_low_toxic_comments.xlsx')\n",
        "calculate_krippendorff_alpha('Sample_toxic_comments.xlsx')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuwS1BmmkgX0",
        "outputId": "a29312ac-b762-4ae6-cf31-2523c94be2c7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Sample_high_toxic_comments.xlsx Krippendorff's Alpha: 0.365\n",
            " Sample_low_toxic_comments.xlsx Krippendorff's Alpha: 0.511\n",
            " Sample_toxic_comments.xlsx Krippendorff's Alpha: 0.615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Calculate Accuracy for Dictionary and ML Methods\n",
        "def calculate_accuracy(file):\n",
        "    df = pd.read_excel(file)  # Load annotated samples\n",
        "\n",
        "    # Calculate ML Method Accuracy\n",
        "    ml_accuracy = accuracy_score(df['label2'], df['label1'])\n",
        "    print(f\"ML Method Accuracy: {ml_accuracy:.3f}\")\n",
        "\n",
        "# Example Usage\n",
        "calculate_accuracy('Sample_toxic_comments - Copy.xlsx')\n"
      ],
      "metadata": {
        "id": "C4-DeZFeklJ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6a6b02f-524a-4969-c1e5-612d568538ef"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ML Method Accuracy: 0.690\n"
          ]
        }
      ]
    }
  ]
}